<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>clip-38s · StatisticalRethinking.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>StatisticalRethinking.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../intro/">Home</a></li><li><a class="toctext" href="../../layout/">Layout</a></li><li><a class="toctext" href="../../versions/">Versions</a></li><li><a class="toctext" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="toctext" href="../../references/">References</a></li><li><span class="toctext">Chapter 00</span><ul><li><a class="toctext" href="../../00/clip-01-03/">clip-01-03</a></li><li><a class="toctext" href="../../00/clip-04-05/">clip-04-05</a></li></ul></li><li><span class="toctext">Chapter 02</span><ul><li><a class="toctext" href="../../02/clip-01-02/">clip-01-02</a></li><li><a class="toctext" href="../../02/clip-03-05/">clip-03-05</a></li><li><a class="toctext" href="../../02/clip-06-07/">clip-06-07</a></li><li><a class="toctext" href="../../02/clip-08/">clip-08</a></li><li><a class="toctext" href="../../02/m2.1s/">m2.1s</a></li></ul></li><li><span class="toctext">Chapter 03</span><ul><li><a class="toctext" href="../../03/clip-01/">clip-01</a></li><li><a class="toctext" href="../../03/clip-02-05/">clip-02-05</a></li><li><a class="toctext" href="../../03/clip-05s/">clip-05s</a></li><li><a class="toctext" href="../../03/clip-06-16s/">clip-06-16s</a></li></ul></li><li><span class="toctext">Chapter 04</span><ul><li><a class="toctext" href="../m4.1s/">m4.1s</a></li><li><a class="toctext" href="../clip-01-06/">clip-01-06</a></li><li><a class="toctext" href="../clip-07-13s/">clip-07-13s</a></li><li><a class="toctext" href="../clip-14-20/">clip-14-20</a></li><li><a class="toctext" href="../clip-21-23/">clip-21-23</a></li><li><a class="toctext" href="../clip-24-29s/">clip-24-29s</a></li><li><a class="toctext" href="../clip-30s/">clip-30s</a></li><li class="current"><a class="toctext" href>clip-38s</a><ul class="internal"><li><a class="toctext" href="#Linear-Regression-1">Linear Regression</a></li></ul></li><li><a class="toctext" href="../clip-43s/">clip-43s</a></li><li><a class="toctext" href="../clip-45-47s/">clip-45-47s</a></li><li><a class="toctext" href="../clip-48-54s/">clip-48-54s</a></li><li><a class="toctext" href="../m4.2s/">m4.2s</a></li><li><a class="toctext" href="../m4.3s/">m4.3s</a></li><li><a class="toctext" href="../m4.4s/">m4.4s</a></li><li><a class="toctext" href="../m4.5s/">m4.5s</a></li></ul></li><li><a class="toctext" href="../../">Functions</a></li></ul></nav><article id="docs"><header><nav><ul><li>Chapter 04</li><li><a href>clip-38s</a></li></ul><a class="edit-page" href="https://github.com/StatisticalRethinkingJulia/StatisticalRethinking.jl/blob/master/scripts/04/clip-38s.jl"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>clip-38s</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id="Linear-Regression-1" href="#Linear-Regression-1">Linear Regression</a></h2><h3><a class="nav-anchor" id="Added-snippet-used-as-a-reference-for-all-models-1" href="#Added-snippet-used-as-a-reference-for-all-models-1">Added snippet used as a reference for all models</a></h3><p>This model is based on the TuringTutorial example <a href="https://github.com/TuringLang/TuringTutorials/blob/csp/linear/LinearRegression.ipynb">LinearRegression</a> by Cameron Pfiffer.</p><p>Turing is powerful when applied to complex hierarchical models, but it can also be put to task at common statistical procedures, like linear regression. This tutorial covers how to implement a linear regression model in Turing.</p><p>We begin by importing all the necessary libraries.</p><div><pre><code class="language-julia">using StatisticalRethinking, CmdStan, StanMCMCChains, GLM
gr(size=(500,500))

ProjDir = rel_path(&quot;..&quot;, &quot;scripts&quot;, &quot;00&quot;)
cd(ProjDir)</code></pre></div><p>Import the dataset.</p><div><pre><code class="language-julia">howell1 = CSV.read(rel_path(&quot;..&quot;, &quot;data&quot;, &quot;Howell1.csv&quot;), delim=&#39;;&#39;)
df = convert(DataFrame, howell1);</code></pre><table class="data-frame"><thead><tr><th></th><th>height</th><th>weight</th><th>age</th><th>male</th></tr><tr><th></th><th>Float64⍰</th><th>Float64⍰</th><th>Float64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>544 rows × 4 columns</p><tr><th>1</th><td>151.765</td><td>47.8256</td><td>63.0</td><td>1</td></tr><tr><th>2</th><td>139.7</td><td>36.4858</td><td>63.0</td><td>0</td></tr><tr><th>3</th><td>136.525</td><td>31.8648</td><td>65.0</td><td>0</td></tr><tr><th>4</th><td>156.845</td><td>53.0419</td><td>41.0</td><td>1</td></tr><tr><th>5</th><td>145.415</td><td>41.2769</td><td>51.0</td><td>0</td></tr><tr><th>6</th><td>163.83</td><td>62.9926</td><td>35.0</td><td>1</td></tr><tr><th>7</th><td>149.225</td><td>38.2435</td><td>32.0</td><td>0</td></tr><tr><th>8</th><td>168.91</td><td>55.48</td><td>27.0</td><td>1</td></tr><tr><th>9</th><td>147.955</td><td>34.8699</td><td>19.0</td><td>0</td></tr><tr><th>10</th><td>165.1</td><td>54.4877</td><td>54.0</td><td>1</td></tr><tr><th>11</th><td>154.305</td><td>49.8951</td><td>47.0</td><td>0</td></tr><tr><th>12</th><td>151.13</td><td>41.2202</td><td>66.0</td><td>1</td></tr><tr><th>13</th><td>144.78</td><td>36.0322</td><td>73.0</td><td>0</td></tr><tr><th>14</th><td>149.9</td><td>47.7</td><td>20.0</td><td>0</td></tr><tr><th>15</th><td>150.495</td><td>33.8493</td><td>65.3</td><td>0</td></tr><tr><th>16</th><td>163.195</td><td>48.5627</td><td>36.0</td><td>1</td></tr><tr><th>17</th><td>157.48</td><td>42.3258</td><td>44.0</td><td>1</td></tr><tr><th>18</th><td>143.942</td><td>38.3569</td><td>31.0</td><td>0</td></tr><tr><th>19</th><td>121.92</td><td>19.6179</td><td>12.0</td><td>1</td></tr><tr><th>20</th><td>105.41</td><td>13.948</td><td>8.0</td><td>0</td></tr><tr><th>21</th><td>86.36</td><td>10.4893</td><td>6.5</td><td>0</td></tr><tr><th>22</th><td>161.29</td><td>48.9879</td><td>39.0</td><td>1</td></tr><tr><th>23</th><td>156.21</td><td>42.7227</td><td>29.0</td><td>0</td></tr><tr><th>24</th><td>129.54</td><td>23.5868</td><td>13.0</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div><p>Use only adults</p><div><pre><code class="language-julia">data = filter(row -&gt; row[:age] &gt;= 18, df)</code></pre><table class="data-frame"><thead><tr><th></th><th>height</th><th>weight</th><th>age</th><th>male</th></tr><tr><th></th><th>Float64⍰</th><th>Float64⍰</th><th>Float64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>352 rows × 4 columns</p><tr><th>1</th><td>151.765</td><td>47.8256</td><td>63.0</td><td>1</td></tr><tr><th>2</th><td>139.7</td><td>36.4858</td><td>63.0</td><td>0</td></tr><tr><th>3</th><td>136.525</td><td>31.8648</td><td>65.0</td><td>0</td></tr><tr><th>4</th><td>156.845</td><td>53.0419</td><td>41.0</td><td>1</td></tr><tr><th>5</th><td>145.415</td><td>41.2769</td><td>51.0</td><td>0</td></tr><tr><th>6</th><td>163.83</td><td>62.9926</td><td>35.0</td><td>1</td></tr><tr><th>7</th><td>149.225</td><td>38.2435</td><td>32.0</td><td>0</td></tr><tr><th>8</th><td>168.91</td><td>55.48</td><td>27.0</td><td>1</td></tr><tr><th>9</th><td>147.955</td><td>34.8699</td><td>19.0</td><td>0</td></tr><tr><th>10</th><td>165.1</td><td>54.4877</td><td>54.0</td><td>1</td></tr><tr><th>11</th><td>154.305</td><td>49.8951</td><td>47.0</td><td>0</td></tr><tr><th>12</th><td>151.13</td><td>41.2202</td><td>66.0</td><td>1</td></tr><tr><th>13</th><td>144.78</td><td>36.0322</td><td>73.0</td><td>0</td></tr><tr><th>14</th><td>149.9</td><td>47.7</td><td>20.0</td><td>0</td></tr><tr><th>15</th><td>150.495</td><td>33.8493</td><td>65.3</td><td>0</td></tr><tr><th>16</th><td>163.195</td><td>48.5627</td><td>36.0</td><td>1</td></tr><tr><th>17</th><td>157.48</td><td>42.3258</td><td>44.0</td><td>1</td></tr><tr><th>18</th><td>143.942</td><td>38.3569</td><td>31.0</td><td>0</td></tr><tr><th>19</th><td>161.29</td><td>48.9879</td><td>39.0</td><td>1</td></tr><tr><th>20</th><td>156.21</td><td>42.7227</td><td>29.0</td><td>0</td></tr><tr><th>21</th><td>146.4</td><td>35.4936</td><td>56.0</td><td>1</td></tr><tr><th>22</th><td>148.59</td><td>37.9033</td><td>45.0</td><td>0</td></tr><tr><th>23</th><td>147.32</td><td>35.4652</td><td>19.0</td><td>0</td></tr><tr><th>24</th><td>147.955</td><td>40.313</td><td>29.0</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div><p>Show the first six rows of the dataset.</p><div><pre><code class="language-julia">first(data, 6)</code></pre><table class="data-frame"><thead><tr><th></th><th>height</th><th>weight</th><th>age</th><th>male</th></tr><tr><th></th><th>Float64⍰</th><th>Float64⍰</th><th>Float64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>6 rows × 4 columns</p><tr><th>1</th><td>151.765</td><td>47.8256</td><td>63.0</td><td>1</td></tr><tr><th>2</th><td>139.7</td><td>36.4858</td><td>63.0</td><td>0</td></tr><tr><th>3</th><td>136.525</td><td>31.8648</td><td>65.0</td><td>0</td></tr><tr><th>4</th><td>156.845</td><td>53.0419</td><td>41.0</td><td>1</td></tr><tr><th>5</th><td>145.415</td><td>41.2769</td><td>51.0</td><td>0</td></tr><tr><th>6</th><td>163.83</td><td>62.9926</td><td>35.0</td><td>1</td></tr></tbody></table></div><p>The next step is to get our data ready for testing. We&#39;ll split the mtcars dataset into two subsets, one for training our model and one for evaluating our model. Then, we separate the labels we want to learn (MPG, in this case) and standardize the datasets by subtracting each column&#39;s means and dividing by the standard deviation of that column.</p><p>The resulting data is not very familiar looking, but this standardization process helps the sampler converge far easier. We also create a function called unstandardize, which returns the standardized values to their original form. We will use this function later on when we make predictions.</p><p>Split our dataset 70%/30% into training/test sets.</p><div><pre><code class="language-julia">n = size(data, 1)
test_ind = sample(1:n, Int(floor(0.3*n)), replace=false);
train_ind = [(i) for i=1:n if !(i in test_ind)];
test = data[test_ind, :];
train = data[train_ind, :];</code></pre><table class="data-frame"><thead><tr><th></th><th>height</th><th>weight</th><th>age</th><th>male</th></tr><tr><th></th><th>Float64⍰</th><th>Float64⍰</th><th>Float64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>247 rows × 4 columns</p><tr><th>1</th><td>151.765</td><td>47.8256</td><td>63.0</td><td>1</td></tr><tr><th>2</th><td>139.7</td><td>36.4858</td><td>63.0</td><td>0</td></tr><tr><th>3</th><td>145.415</td><td>41.2769</td><td>51.0</td><td>0</td></tr><tr><th>4</th><td>163.83</td><td>62.9926</td><td>35.0</td><td>1</td></tr><tr><th>5</th><td>149.225</td><td>38.2435</td><td>32.0</td><td>0</td></tr><tr><th>6</th><td>168.91</td><td>55.48</td><td>27.0</td><td>1</td></tr><tr><th>7</th><td>147.955</td><td>34.8699</td><td>19.0</td><td>0</td></tr><tr><th>8</th><td>151.13</td><td>41.2202</td><td>66.0</td><td>1</td></tr><tr><th>9</th><td>144.78</td><td>36.0322</td><td>73.0</td><td>0</td></tr><tr><th>10</th><td>149.9</td><td>47.7</td><td>20.0</td><td>0</td></tr><tr><th>11</th><td>150.495</td><td>33.8493</td><td>65.3</td><td>0</td></tr><tr><th>12</th><td>163.195</td><td>48.5627</td><td>36.0</td><td>1</td></tr><tr><th>13</th><td>157.48</td><td>42.3258</td><td>44.0</td><td>1</td></tr><tr><th>14</th><td>161.29</td><td>48.9879</td><td>39.0</td><td>1</td></tr><tr><th>15</th><td>156.21</td><td>42.7227</td><td>29.0</td><td>0</td></tr><tr><th>16</th><td>148.59</td><td>37.9033</td><td>45.0</td><td>0</td></tr><tr><th>17</th><td>147.955</td><td>40.313</td><td>29.0</td><td>1</td></tr><tr><th>18</th><td>146.05</td><td>37.5064</td><td>24.0</td><td>0</td></tr><tr><th>19</th><td>146.05</td><td>38.4986</td><td>35.0</td><td>0</td></tr><tr><th>20</th><td>152.705</td><td>46.6066</td><td>33.0</td><td>0</td></tr><tr><th>21</th><td>142.875</td><td>38.8388</td><td>27.0</td><td>0</td></tr><tr><th>22</th><td>142.875</td><td>35.5786</td><td>32.0</td><td>0</td></tr><tr><th>23</th><td>147.955</td><td>47.4004</td><td>36.0</td><td>0</td></tr><tr><th>24</th><td>160.655</td><td>47.8823</td><td>24.0</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div><p>Save dataframe versions of our dataset.</p><div><pre><code class="language-julia">train_cut = DataFrame(train)
test_cut = DataFrame(test)</code></pre><table class="data-frame"><thead><tr><th></th><th>height</th><th>weight</th><th>age</th><th>male</th></tr><tr><th></th><th>Float64⍰</th><th>Float64⍰</th><th>Float64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>105 rows × 4 columns</p><tr><th>1</th><td>145.415</td><td>44.9907</td><td>37.0</td><td>0</td></tr><tr><th>2</th><td>142.875</td><td>37.3363</td><td>39.0</td><td>0</td></tr><tr><th>3</th><td>150.495</td><td>40.4831</td><td>68.0</td><td>0</td></tr><tr><th>4</th><td>167.005</td><td>55.1965</td><td>42.0</td><td>1</td></tr><tr><th>5</th><td>138.43</td><td>39.094</td><td>23.0</td><td>0</td></tr><tr><th>6</th><td>157.988</td><td>48.591</td><td>28.0</td><td>1</td></tr><tr><th>7</th><td>159.385</td><td>47.2019</td><td>28.0</td><td>1</td></tr><tr><th>8</th><td>159.385</td><td>50.1786</td><td>63.0</td><td>1</td></tr><tr><th>9</th><td>140.335</td><td>37.4497</td><td>22.0</td><td>0</td></tr><tr><th>10</th><td>153.035</td><td>39.9728</td><td>49.5</td><td>0</td></tr><tr><th>11</th><td>165.1</td><td>51.1992</td><td>49.0</td><td>1</td></tr><tr><th>12</th><td>156.21</td><td>39.2924</td><td>26.0</td><td>1</td></tr><tr><th>13</th><td>167.005</td><td>52.9002</td><td>32.0</td><td>1</td></tr><tr><th>14</th><td>153.67</td><td>41.3336</td><td>27.0</td><td>0</td></tr><tr><th>15</th><td>167.64</td><td>50.6889</td><td>57.0</td><td>1</td></tr><tr><th>16</th><td>159.385</td><td>42.9778</td><td>43.0</td><td>1</td></tr><tr><th>17</th><td>154.305</td><td>49.8951</td><td>47.0</td><td>0</td></tr><tr><th>18</th><td>162.56</td><td>45.9545</td><td>35.0</td><td>1</td></tr><tr><th>19</th><td>141.605</td><td>44.2252</td><td>60.0</td><td>0</td></tr><tr><th>20</th><td>155.575</td><td>54.3176</td><td>37.0</td><td>0</td></tr><tr><th>21</th><td>152.4</td><td>46.72</td><td>44.0</td><td>0</td></tr><tr><th>22</th><td>148.59</td><td>43.885</td><td>33.0</td><td>0</td></tr><tr><th>23</th><td>165.1</td><td>54.4877</td><td>54.0</td><td>1</td></tr><tr><th>24</th><td>153.035</td><td>39.5476</td><td>33.0</td><td>0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div><p>Create our labels. These are the values we are trying to predict.</p><div><pre><code class="language-julia">train_label = train[:, :height]
test_label = test[:, :height]</code></pre><pre><code class="language-none">105-element Array{Union{Missing, Float64},1}:
 145.415
 142.875
 150.495
 167.005
 138.43
 157.988
 159.385
 159.385
 140.335
 153.035
   ⋮
 154.305
 147.32
 170.815
 147.955
 160.655
 153.035
 143.9418
 160.655
 161.925</code></pre></div><p>Get the list of columns to keep.</p><div><pre><code class="language-julia">remove_names = filter(x-&gt;!in(x, [:height, :age, :male]), names(data))</code></pre><pre><code class="language-none">1-element Array{Symbol,1}:
 :weight</code></pre></div><p>Filter the test and train sets.</p><div><pre><code class="language-julia">train = Matrix(train[:, remove_names]);
test = Matrix(test[:, remove_names]);</code></pre><pre><code class="language-none">105×1 Array{Union{Missing, Float64},2}:
 44.9906565
 37.3362915
 40.483086
 55.1964765
 39.0939605
 48.591043
 47.2019175
 50.178615
 37.4496895
 39.972795
  ⋮
 48.874538
 51.255896
 59.760746
 36.4858065
 55.3665735
 49.5832755
 38.3568735
 48.5059945
 56.9541455</code></pre></div><p>A handy helper function to rescale our dataset.</p><div><pre><code class="language-julia">function standardize(x)
    return (x .- mean(x, dims=1)) ./ std(x, dims=1), x
end</code></pre><pre><code class="language-none">standardize (generic function with 1 method)</code></pre></div><p>Another helper function to unstandardize our datasets.</p><div><pre><code class="language-julia">function unstandardize(x, orig)
    return x .* std(orig, dims=1) .+ mean(orig, dims=1)
end</code></pre><pre><code class="language-none">unstandardize (generic function with 1 method)</code></pre></div><p>Standardize our dataset.</p><div><pre><code class="language-julia">(train, train_orig) = standardize(train)
(test, test_orig) = standardize(test)
(train_label, train_l_orig) = standardize(train_label)
(test_label, test_l_orig) = standardize(test_label);</code></pre><pre><code class="language-none">([-1.21549, -1.54632, -0.55384, 1.59653, -2.12527, 0.422099, 0.604053, 0.604053, -1.87715, -0.223013  …  0.604053, -0.0575998, -0.967373, 2.09277, -0.884666, 0.769467, -0.223013, -1.40737, 0.769467, 0.93488], Union{Missing, Float64}[145.415, 142.875, 150.495, 167.005, 138.43, 157.988, 159.385, 159.385, 140.335, 153.035  …  159.385, 154.305, 147.32, 170.815, 147.955, 160.655, 153.035, 143.942, 160.655, 161.925])</code></pre></div><p>Design matrix</p><div><pre><code class="language-julia">dmat = [ones(size(train, 1)) train]</code></pre><pre><code class="language-none">247×2 Array{Float64,2}:
 1.0   0.446711
 1.0  -1.32921
 1.0  -0.578883
 1.0   2.822
 1.0  -1.05394
 1.0   1.64546
 1.0  -1.58228
 1.0  -0.587763
 1.0  -1.40025
 1.0   0.42704
 ⋮
 1.0  -0.134903
 1.0  -0.148223
 1.0   0.455591
 1.0  -0.87191
 1.0  -0.614402
 1.0   0.322396
 1.0  -1.67995
 1.0   1.126
 1.0   1.42347</code></pre></div><p>Bayesian linear regression.</p><div><pre><code class="language-julia">lrmodel = &quot;
data {
  int N; //the number of observations
  int K; //the number of columns in the model matrix
  real y[N]; //the response
  matrix[N,K] X; //the model matrix
}
parameters {
  vector[K] beta; //the regression parameters
  real sigma; //the standard deviation
}
transformed parameters {
  vector[N] linpred;
  linpred &lt;- X*beta;
}
model {
  beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008

  for(i in 2:K)
   beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008

  y ~ normal(linpred,sigma);
}
&quot;;</code></pre><pre><code class="language-none">&quot;\ndata {\n  int N; //the number of observations\n  int K; //the number of columns in the model matrix\n  real y[N]; //the response\n  matrix[N,K] X; //the model matrix\n}\nparameters {\n  vector[K] beta; //the regression parameters\n  real sigma; //the standard deviation\n}\ntransformed parameters {\n  vector[N] linpred;\n  linpred &lt;- X*beta;\n}\nmodel {\n  beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008\n\n  for(i in 2:K)\n   beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008\n\n  y ~ normal(linpred,sigma);\n}\n&quot;</code></pre></div><p>Define the Stanmodel and set the output format to :mcmcchains.</p><div><pre><code class="language-julia">stanmodel = Stanmodel(name=&quot;linear_regression&quot;,
  monitors = [&quot;beta.1&quot;, &quot;beta.2&quot;, &quot;sigma&quot;],
  model=lrmodel);</code></pre></div><p>Input data for cmdstan</p><div><pre><code class="language-julia">lrdata = Dict(&quot;N&quot; =&gt; size(train, 1), &quot;K&quot; =&gt; size(dmat, 2), &quot;y&quot; =&gt; train_label, &quot;X&quot; =&gt; dmat);</code></pre><pre><code class="language-none">
File /home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan will be updated.

Dict{String,Any} with 4 entries:
  &quot;X&quot; =&gt; [1.0 0.446711; 1.0 -1.32921; … ; 1.0 1.126; 1.0 1.42347]
  &quot;N&quot; =&gt; 247
  &quot;K&quot; =&gt; 2
  &quot;y&quot; =&gt; [-0.355623, -1.90555, -1.17137, 1.1943, -0.681923, 1.8469, -0.845072, …</code></pre></div><p>Sample using cmdstan</p><div><pre><code class="language-julia">rc, sim, cnames = stan(stanmodel, lrdata, ProjDir, diagnostics=false,
  summary=false, CmdStanDir=CMDSTAN_HOME);</code></pre></div><p>Convert to a  Chain object</p><div><pre><code class="language-julia">cnames = [&quot;alpha&quot;, &quot;beta[1]&quot;, &quot;sigma&quot;]
chain = convert_a3d(sim, cnames, Val(:mcmcchains))</code></pre><pre><code class="language-none">Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -50.4778, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -52.0852, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -12.0986, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -2.48821, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -133.08, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -2.90313, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -7.38489, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -46.6922, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -0.143536, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -45.9445, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -10.6355, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -1.46467, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -5832.39, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -140.195, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -6.7054, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -0.399867, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -1.68461, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -2.47018, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -0.531205, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -1.37717, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Exception: normal_lpdf: Scale parameter is -0.427916, but must be &gt; 0!  (in &#39;/home/travis/build/StatisticalRethinkingJulia/StatisticalRethinking.jl/docs/build/04/tmp/linear_regression.stan&#39; at line 21)

If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

Object of type Chains, with data of type 1000×3×4 Array{Float64,3}

Log evidence      = 0.0
Iterations        = 1:1000
Thinning interval = 1
Chains            = 1, 2, 3, 4
Samples per chain = 1000
internals         =
parameters        = alpha, beta[1], sigma

parameters
         Mean    SD   Naive SE  MCSE   ESS
  alpha 0.0018 0.0412   0.0007 0.0006 1000
beta[1] 0.7626 0.0412   0.0007 0.0007 1000
  sigma 0.6503 0.0298   0.0005 0.0005 1000</code></pre></div><p>Describe the chains.</p><div><pre><code class="language-julia">describe(chain)</code></pre><pre><code class="language-none">Log evidence      = 0.0
Iterations        = 1:1000
Thinning interval = 1
Chains            = 1, 2, 3, 4
Samples per chain = 1000
parameters        = alpha, beta[1], sigma

Empirical Posterior Estimates
────────────────────────────────────────────
parameters
         Mean    SD   Naive SE  MCSE   ESS
  alpha 0.0018 0.0412   0.0007 0.0006 1000
beta[1] 0.7626 0.0412   0.0007 0.0007 1000
  sigma 0.6503 0.0298   0.0005 0.0005 1000

Quantiles
────────────────────────────────────────────
parameters
          2.5%   25.0%   50.0%  75.0%  97.5%
  alpha -0.1434 -0.0260 0.0018 0.0298 0.1583
beta[1]  0.5708  0.7342 0.7633 0.7902 0.9740
  sigma  0.5631  0.6294 0.6493 0.6696 0.7627</code></pre></div><p>Perform multivariate OLS.</p><div><pre><code class="language-julia">ols = lm(@formula(height ~ weight), train_cut)</code></pre><pre><code class="language-none">StatsModels.DataFrameRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}

Formula: height ~ 1 + weight

Coefficients:
             Estimate Std.Error t value Pr(&gt;|t|)
(Intercept)   112.679   2.28514 49.3096   &lt;1e-99
weight       0.930644 0.0503085 18.4987   &lt;1e-47</code></pre></div><p>Store our predictions in the original dataframe.</p><div><pre><code class="language-julia">train_cut.OLSPrediction = predict(ols);
test_cut.OLSPrediction = predict(ols, test_cut);</code></pre><pre><code class="language-none">105-element Array{Union{Missing, Float64},1}:
 154.5494936740149
 147.4260041906831
 150.35454986716394
 164.04747965179072
 149.06176844241114
 157.90017206061916
 156.60739063586635
 159.37763654605095
 147.53153736821392
 149.87965056827517
   ⋮
 158.16400500444627
 160.38020173259395
 168.29519004740712
 146.63450535920177
 164.205779418087
 158.82358736401403
 148.37580278846065
 157.82102217747104
 165.68324390351876</code></pre></div><p>Make a prediction given an input vector.</p><div><pre><code class="language-julia">function prediction(chain, x)
    α = chain.value[:, 1, :];
    β = [chain.value[:, i, :] for i in 2:2];
    return  mean(α) .+ x * mean.(β)
end</code></pre><pre><code class="language-none">prediction (generic function with 1 method)</code></pre></div><p>Calculate the predictions for the training and testing sets.</p><div><pre><code class="language-julia">train_cut.BayesPredictions = unstandardize(prediction(chain, train), train_l_orig);
test_cut.BayesPredictions = unstandardize(prediction(chain, test), test_l_orig);</code></pre><pre><code class="language-none">105-element Array{Float64,1}:
 154.72510781615944
 147.98804038003126
 150.7577236593284
 163.70786439766368
 149.53507068017922
 157.8940247213012
 156.67137174215202
 159.29134241175743
 148.08784878649243
 150.3085858302532
   ⋮
 158.1435457374541
 160.23952227313845
 167.7251527577253
 147.23947733157257
 163.85757700735542
 158.76734827783636
 148.8863160381817
 157.81916841645534
 165.25489469781164</code></pre></div><p>Show the first side rows of the modified dataframe.</p><div><pre><code class="language-julia">remove_names = filter(x-&gt;!in(x, [:age, :male]), names(test_cut));
test_cut = test_cut[remove_names];
first(test_cut, 6)

bayes_loss1 = sum((train_cut.BayesPredictions - train_cut.height).^2);
ols_loss1 = sum((train_cut.OLSPrediction - train_cut.height).^2);

bayes_loss2 = sum((test_cut.BayesPredictions - test_cut.height).^2);
ols_loss2 = sum((test_cut.OLSPrediction - test_cut.height).^2);

println(&quot;\nTraining set:&quot;)
println(&quot;  Bayes loss: $bayes_loss1&quot;)
println(&quot;  OLS loss: $ols_loss1&quot;)

println(&quot;Test set:&quot;)
println(&quot;  Bayes loss: $bayes_loss2&quot;)
println(&quot;  OLS loss: $ols_loss2&quot;)</code></pre><pre><code class="language-none">
Training set:
  Bayes loss: 6219.4437353221965
  OLS loss: 6219.389121494657
Test set:
  Bayes loss: 2816.945395395016
  OLS loss: 2845.801069922776</code></pre></div><p>Plot the chains.</p><div><pre><code class="language-julia">#plot(chain)</code></pre></div><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><footer><hr/><a class="previous" href="../clip-30s/"><span class="direction">Previous</span><span class="title">clip-30s</span></a><a class="next" href="../clip-43s/"><span class="direction">Next</span><span class="title">clip-43s</span></a></footer></article></body></html>
